{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "phase4-seq-2-seq",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2vOYq2-FOZk"
      },
      "source": [
        "# **Importing Libraries and Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E0fhfjVt69e",
        "outputId": "e9ee1a3a-e5b8-4d3d-a394-47b46aa0f0c9"
      },
      "source": [
        "!python -m spacy download en --quiet\n",
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\n",
        "INDIC_NLP_RESOURCES=\"/content/indic_nlp_resources\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import spacy\n",
        "import random\n",
        "import string\n",
        "import csv\n",
        "import sys\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
        "\n",
        "from indicnlp import common\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "\n",
        "\n",
        "from indicnlp import common\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 1271, done.\u001b[K\n",
            "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 1271 (delta 50), reused 54 (delta 25), pack-reused 1178\u001b[K\n",
            "Receiving objects: 100% (1271/1271), 9.56 MiB | 17.14 MiB/s, done.\n",
            "Resolving deltas: 100% (654/654), done.\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 133, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 133 (delta 0), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (133/133), 149.77 MiB | 31.54 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI_IDzqeFb8G"
      },
      "source": [
        "# **Mounting Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1bip0Ty4B_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1b9a002-056a-4eef-b69c-5afb9f7baf0f"
      },
      "source": [
        "from google.colab import drive                       \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHiRzQFpGNqP"
      },
      "source": [
        "# **Reading data and Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlGaeZEQzvD9"
      },
      "source": [
        "# reading the dataset from google drive                     \n",
        "\n",
        "with open('/content/drive/MyDrive/AssignmentNLP/train/train.csv', newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    raw_data = list(reader)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kfxa_EIeMrNs"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")             #setting device as GPU if available, else device -> CPU\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxAsplUi7UHR"
      },
      "source": [
        "train_data=raw_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVMvcgPA6SdZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71e3befd-0dcd-4462-ffbb-55495ec7fd56"
      },
      "source": [
        "for i in range(0,10):\n",
        "  print(train_data[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'hindi', 'english']\n",
            "['0', 'एल सालवाडोर मे, जिन दोनो पक्षों ने सिविल-युद्ध से वापसी ली, उन्होंने वही काम किये जो कैदियों की कश्मकश के निदान हैं।', \"In El Salvador, both sides that withdrew from their civil war took moves that had been proven to mirror a prisoner's dilemma strategy.\"]\n",
            "['1', 'मैं उनके साथ कोई लेना देना नहीं है.', 'I have nothing to do with them.']\n",
            "['2', '-हटाओ रिक.', 'Fuck them, Rick.']\n",
            "['3', 'क्योंकि यह एक खुशियों भरी फ़िल्म है.', \"Because it's a happy film.\"]\n",
            "['4', 'The thought reaching the eyes...', 'The thought reaching the eyes...']\n",
            "['5', 'मैंने तुमे School से हटवा दिया .', 'I got you suspended.']\n",
            "['6', 'यह Vika, एक फूल है.', \"It's a flower, Vika.\"]\n",
            "['7', 'पर मेरे लिए उसका यहुदी विरोधी होना उसके कार्यों को और भी प्रशंसनीय बनाता है क्योंकि उसके पास भी पक्षपात करने के वही कारण थे जो बाकी फौजियों के पास थे पर उसकी सच जानने और उसे बनाए रखने की प्रेरणा सबसे ऊपर थी', 'But personally, for me, the fact that Picquart was anti-Semitic actually makes his actions more admirable, because he had the same prejudices, the same reasons to be biased as his fellow officers, but his motivation to find the truth and uphold it trumped all of that.']\n",
            "['8', 'नहीं, नहीं, नहीं... ठीक है, हम उह हूँ... हम कार्ड का उपयोग करेंगे.', \"No, no, no... fine, we'll uh... we'll use the card.\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK1nsbEy6lY0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a737c11a-db21-44d7-f9e9-4a8f4eff0efe"
      },
      "source": [
        "train_data=train_data[1:]             #removing header\n",
        "for i in range(0,10):\n",
        "  print(train_data[i])\n",
        "for i in range(0, len(train_data)):               #extracting the (hindi,english) from the two-dimensional list read from the train.csv file\n",
        "  train_data[i]=train_data[i][1:]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['0', 'एल सालवाडोर मे, जिन दोनो पक्षों ने सिविल-युद्ध से वापसी ली, उन्होंने वही काम किये जो कैदियों की कश्मकश के निदान हैं।', \"In El Salvador, both sides that withdrew from their civil war took moves that had been proven to mirror a prisoner's dilemma strategy.\"]\n",
            "['1', 'मैं उनके साथ कोई लेना देना नहीं है.', 'I have nothing to do with them.']\n",
            "['2', '-हटाओ रिक.', 'Fuck them, Rick.']\n",
            "['3', 'क्योंकि यह एक खुशियों भरी फ़िल्म है.', \"Because it's a happy film.\"]\n",
            "['4', 'The thought reaching the eyes...', 'The thought reaching the eyes...']\n",
            "['5', 'मैंने तुमे School से हटवा दिया .', 'I got you suspended.']\n",
            "['6', 'यह Vika, एक फूल है.', \"It's a flower, Vika.\"]\n",
            "['7', 'पर मेरे लिए उसका यहुदी विरोधी होना उसके कार्यों को और भी प्रशंसनीय बनाता है क्योंकि उसके पास भी पक्षपात करने के वही कारण थे जो बाकी फौजियों के पास थे पर उसकी सच जानने और उसे बनाए रखने की प्रेरणा सबसे ऊपर थी', 'But personally, for me, the fact that Picquart was anti-Semitic actually makes his actions more admirable, because he had the same prejudices, the same reasons to be biased as his fellow officers, but his motivation to find the truth and uphold it trumped all of that.']\n",
            "['8', 'नहीं, नहीं, नहीं... ठीक है, हम उह हूँ... हम कार्ड का उपयोग करेंगे.', \"No, no, no... fine, we'll uh... we'll use the card.\"]\n",
            "['9', '- क्या भाषा क्या वे वहाँ बात की?', '- What language do they speak there?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cq3gYepc7svb",
        "outputId": "05cc8850-9fef-484d-bcfc-5d7731ebc3d5"
      },
      "source": [
        "for i in range(0,10):\n",
        "  print(train_data[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['एल सालवाडोर मे, जिन दोनो पक्षों ने सिविल-युद्ध से वापसी ली, उन्होंने वही काम किये जो कैदियों की कश्मकश के निदान हैं।', \"In El Salvador, both sides that withdrew from their civil war took moves that had been proven to mirror a prisoner's dilemma strategy.\"]\n",
            "['मैं उनके साथ कोई लेना देना नहीं है.', 'I have nothing to do with them.']\n",
            "['-हटाओ रिक.', 'Fuck them, Rick.']\n",
            "['क्योंकि यह एक खुशियों भरी फ़िल्म है.', \"Because it's a happy film.\"]\n",
            "['The thought reaching the eyes...', 'The thought reaching the eyes...']\n",
            "['मैंने तुमे School से हटवा दिया .', 'I got you suspended.']\n",
            "['यह Vika, एक फूल है.', \"It's a flower, Vika.\"]\n",
            "['पर मेरे लिए उसका यहुदी विरोधी होना उसके कार्यों को और भी प्रशंसनीय बनाता है क्योंकि उसके पास भी पक्षपात करने के वही कारण थे जो बाकी फौजियों के पास थे पर उसकी सच जानने और उसे बनाए रखने की प्रेरणा सबसे ऊपर थी', 'But personally, for me, the fact that Picquart was anti-Semitic actually makes his actions more admirable, because he had the same prejudices, the same reasons to be biased as his fellow officers, but his motivation to find the truth and uphold it trumped all of that.']\n",
            "['नहीं, नहीं, नहीं... ठीक है, हम उह हूँ... हम कार्ड का उपयोग करेंगे.', \"No, no, no... fine, we'll uh... we'll use the card.\"]\n",
            "['- क्या भाषा क्या वे वहाँ बात की?', '- What language do they speak there?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mOjwihR7xYe",
        "outputId": "cfa59d40-32ae-42d7-99f4-2c7baf9a6856"
      },
      "source": [
        "print(len(train_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "102322\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh0ttO5f84fp"
      },
      "source": [
        "def standardize(s):                                    # this function is used to standardize the string i.e remove extra extra outer whitespaces, change to lowercase, remove most punctuation marks\n",
        "  s=s.strip()\n",
        "  s=s.lower()\n",
        "  norm_s=\"\"\n",
        "  ignore=string.punctuation                           # this gives all punctuations\n",
        "  for i in s:\n",
        "    #if i=='!' or i=='.' or i=='?' or i==',' or i=='\"' or i==\",\" or i==';' or i==':':\n",
        "    if i in ignore:                                                          \n",
        "      continue\n",
        "    else:\n",
        "      norm_s=norm_s+i\n",
        "\n",
        "  return norm_s                                   # returns the staandardized string\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6rMsqxjFnG5"
      },
      "source": [
        "# **Defining Tokenizers for English (spacy) and Hindi (Indic NLP)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wut4cYmXd2n6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29b4189c-3433-440d-f72a-c0997f26e3fd"
      },
      "source": [
        "from indicnlp.tokenize import indic_tokenize  \n",
        "\n",
        "indic_string='सुनो, कुछ आवाज़ आ रही है। फोन?'\n",
        "\n",
        "print('Input String: {}'.format(indic_string))\n",
        "print('Tokens: ')\n",
        "for t in indic_tokenize.trivial_tokenize(indic_string):          # Inddic NLP tokenizer function()\n",
        "    print(t)\n",
        "\n",
        "print(indic_tokenize.trivial_tokenize(indic_string))\n",
        "\n",
        "def tokenize_hindi(text):                      #tokenizer for hindi using Indic NLP\n",
        "  return indic_tokenize.trivial_tokenize(text)\n",
        "\n",
        "sample_text = 'सुनो, कुछ आवाज़ आ रही है। फोन?'                          # sample tokenization of a hindi sentence (for testing)\n",
        "print(tokenize_hindi(sample_text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input String: सुनो, कुछ आवाज़ आ रही है। फोन?\n",
            "Tokens: \n",
            "सुनो\n",
            ",\n",
            "कुछ\n",
            "आवाज़\n",
            "आ\n",
            "रही\n",
            "है\n",
            "।\n",
            "फोन\n",
            "?\n",
            "['सुनो', ',', 'कुछ', 'आवाज़', 'आ', 'रही', 'है', '।', 'फोन', '?']\n",
            "['सुनो', ',', 'कुछ', 'आवाज़', 'आ', 'रही', 'है', '।', 'फोन', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtn9o6ejefvy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "350d80a6-4308-4036-dc3c-14ff45e9fec1"
      },
      "source": [
        "spacy_english = spacy.load(\"en\")                # importing the spacy pipeline for the English language \"en\"\n",
        "\n",
        "def tokenize_english(text):                  #tokenizer for english using Spacy\n",
        "  return [token.text for token in spacy_english.tokenizer(text)]\n",
        "\n",
        "\n",
        "sample_text = \"I am, going to work\"                        # sample tokenization of an english sentence\n",
        "print(tokenize_english(sample_text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'am', ',', 'going', 'to', 'work']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqnB8H3YB1UH"
      },
      "source": [
        "for i in train_data:                       # iterating thorugh the (hindi, english) sentence pair\n",
        "  i[0]=standardize(i[0])                                 #standardizing the hindi sentence, please refer ti standard() function a few cells above\n",
        "  i[1]=standardize(i[1])                                 # standardizing the english sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kPhH0uBCDbC",
        "outputId": "a1ac7ad4-8ef6-4861-eae2-cb4ff8feefd1"
      },
      "source": [
        "for i in range(0,10):\n",
        "  print(train_data[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['एल सालवाडोर मे जिन दोनो पक्षों ने सिविलयुद्ध से वापसी ली उन्होंने वही काम किये जो कैदियों की कश्मकश के निदान हैं।', 'in el salvador both sides that withdrew from their civil war took moves that had been proven to mirror a prisoners dilemma strategy']\n",
            "['मैं उनके साथ कोई लेना देना नहीं है', 'i have nothing to do with them']\n",
            "['हटाओ रिक', 'fuck them rick']\n",
            "['क्योंकि यह एक खुशियों भरी फ़िल्म है', 'because its a happy film']\n",
            "['the thought reaching the eyes', 'the thought reaching the eyes']\n",
            "['मैंने तुमे school से हटवा दिया ', 'i got you suspended']\n",
            "['यह vika एक फूल है', 'its a flower vika']\n",
            "['पर मेरे लिए उसका यहुदी विरोधी होना उसके कार्यों को और भी प्रशंसनीय बनाता है क्योंकि उसके पास भी पक्षपात करने के वही कारण थे जो बाकी फौजियों के पास थे पर उसकी सच जानने और उसे बनाए रखने की प्रेरणा सबसे ऊपर थी', 'but personally for me the fact that picquart was antisemitic actually makes his actions more admirable because he had the same prejudices the same reasons to be biased as his fellow officers but his motivation to find the truth and uphold it trumped all of that']\n",
            "['नहीं नहीं नहीं ठीक है हम उह हूँ हम कार्ड का उपयोग करेंगे', 'no no no fine well uh well use the card']\n",
            "[' क्या भाषा क्या वे वहाँ बात की', ' what language do they speak there']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAP9CaMSF-fP"
      },
      "source": [
        "# **Creating the English and Hindi Vocabuaries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPuCg2E7ufn1"
      },
      "source": [
        "\n",
        "class Vocabulary:                                                        # defining the classs which will be used to create the vocabularies of hindi and elish lanuguages respectively\n",
        "    def __init__(self, name):\n",
        "        self.name = name                                                 # attribute name to store the name of the vocabulary\n",
        "        self.word_count = dict()                                         # this is a simple look up table ( dictionary) which will be used to keep the frquency count of a word as ot appears in the vocabulary\n",
        "        self.word2index = {\"<sos>\": 0, \"<eos>\": 1}                       # this dictionary will be used to create a mapping from words (string) to unique numbers or indices in the vocabulary\n",
        "        self.index2word = {0: \"<sos>\", 1: \"<eos>\"}                      # this dictionay will be used to create an inverse mapping of word2index, i.e maps indices to string words\n",
        "        self.total_words = 2                                           # this attribute gives the total number of words in the vocabulary, initialized to 2 as, every vocabulary will have start-of-sentence and end-of-sentence token\n",
        "\n",
        "    def processSentence(self, sentence):                       # this fucntions reads a sentence\n",
        "        if self.name==\"hindi\":                                 \n",
        "          list_words=tokenize_hindi(sentence)                        # use hindi tokenizer if calling object is hindi i.e. hindi vocab\n",
        "        else:\n",
        "          list_words=tokenize_english(sentence)                        # use english tokenizer if calling object is english i.e. english vocab\n",
        "        for i in list_words:\n",
        "            self.processWord(i)\n",
        "\n",
        "    def processWord(self, word):                                                    #every encountered word \n",
        "        if word not in self.word2index:                                     # if word encountered does not already exist in vicabulary\n",
        "            self.word2index[word] = self.total_words                           # assign an unique index to the word                      \n",
        "            self.word_count[word] = 1                                  # set word frequency count as 1\n",
        "            self.index2word[self.total_words] = word                  # store the reverse mapping of assigned index to the word \n",
        "            self.total_words += 1                                    # increment total words in the vocabulary by 1\n",
        "        else:\n",
        "            self.word_count[word] += 1                                     # if word already exsists in vocabulary, just increment the frequency count by 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Dd2lJYQC8P3"
      },
      "source": [
        "hindi = Vocabulary(\"hindi\")                                   # instantiating the hind vocabulary\n",
        "english = Vocabulary(\"english\")                              # instantiating the english vocabulary\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyDF4aDdDR1t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "618f40d9-55e0-4ce4-9039-dd2cbdd5cd82"
      },
      "source": [
        "def checkNoise(x):                                   # this function can be used to remove noisy (hindi, english) sentence pairs \n",
        "  l1=x[0].count(\" \")                                \n",
        "  l2=x[1].count(\" \")\n",
        "  if l1<30 and l2<30:                                         # this condition checks if both the sentences are not exceedingly large (i.e. > 30 words or tokens)\n",
        "    if l1>2 and l2>2:                                          # this condition checks that the sentences are atleast >2 words in length, else it might be noise\n",
        "      if l1<2*l2 and l2<2*l1:                                  # this condition checks that the hindi and english sentences are similar in size i.e less than 2 twice the length of each other       \n",
        "        return True                                           # if all the above condition pass, then sentence pair is not noise\n",
        "  return False           #else noise\n",
        "\n",
        "\n",
        "clean_data=list()\n",
        "for i in train_data:\n",
        "  if(checkNoise(i)):                           #  here we de noisify the dataset\n",
        "    clean_data.append(i) \n",
        "\n",
        "print(len(clean_data)) # printing length of the cleaned dataset\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "69589\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZNgU4p_yj4Y",
        "outputId": "2146f33d-f195-4876-9422-85456b8cdc67"
      },
      "source": [
        "for i in range(0,20):                               # checking clean data set\n",
        "  print(clean_data[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['एल सालवाडोर मे जिन दोनो पक्षों ने सिविलयुद्ध से वापसी ली उन्होंने वही काम किये जो कैदियों की कश्मकश के निदान हैं।', 'in el salvador both sides that withdrew from their civil war took moves that had been proven to mirror a prisoners dilemma strategy']\n",
            "['मैं उनके साथ कोई लेना देना नहीं है', 'i have nothing to do with them']\n",
            "['क्योंकि यह एक खुशियों भरी फ़िल्म है', 'because its a happy film']\n",
            "['the thought reaching the eyes', 'the thought reaching the eyes']\n",
            "['यह vika एक फूल है', 'its a flower vika']\n",
            "['नहीं नहीं नहीं ठीक है हम उह हूँ हम कार्ड का उपयोग करेंगे', 'no no no fine well uh well use the card']\n",
            "[' क्या भाषा क्या वे वहाँ बात की', ' what language do they speak there']\n",
            "['उससे बदतर हमारे पेशे ने कानून को जटिलता का चोगा पहना दिया है।', 'worse our profession has shrouded law in a cloak of complexity']\n",
            "['♪औरमैंउसे वहाँखड़े देखा थाएक ', '♪ and i saw her standing there ♪']\n",
            "['बकवास आप क्या कर रहे हैं', 'what the fuck are you']\n",
            "['क्या आपको याद है जब हमने देखा है डौग कि प्रतिमा पर impaled गद्दे', 'do you remember when we saw dougs mattress impaled on that statue']\n",
            "['कोई प्यार के लिए एडी', 'no love for eddie']\n",
            "['अच्छा विचार  अच्छा फोन छत पर ', 'good idea good call on the roof']\n",
            "['यह एक बहुत आसान हो गया होता एक सप्ताह पहले।', 'this would have been a lot easier a week ago']\n",
            "['नान्केट जहां पेक्वॉड जलयात्रा करता है वह खूनी वैश्विक उद्योग का आकर्षक का केंद्र था जिसने दुनिया की व्हेल आबादी का पतन किया।', 'nantucket where the pequod sets sail was the epicenter of this lucrative and bloody global industry which decimated the world’s whale populations']\n",
            "['वह उसे केवल अफ़्रीकी लोगों के लिए सीमित नहीं करती।', 'she doesnt just limit it to black people']\n",
            "['कानून मानव जाति के लिए लागू होता है', 'the law applies to human beings']\n",
            "['मेरे ख्याल से इसमा मतलब ’समुद्र की दैवीय शक्ति’ है एक पोलिनेशियाई भाषा में', 'i think this means divine power of the ocean in a polynesian language']\n",
            "['और पिछली बार जब उसने बुलाया था तो उसने दो दार्शनिकों को भी बुलाया था', 'and the last time the rich guy invited us he also invited a couple of philosophers']\n",
            "['यह बनावट ऐसे लम्बे पुलों को सहारा देने के लिए आदर्श थी जिन्हें थोड़े छोटे लंबरूप तारों से लटकना था।', 'this design was ideal for supporting long decks which hung from smaller vertical cables']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP4ojOCmzwIW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "f3967766-2e47-47f5-cb3f-5b4ddd794254"
      },
      "source": [
        "for sample in clean_data:                                                           # please note executing this cell will take some time (around 45 minutes)\n",
        "  hindi.processSentence(sample[0])                                                 # now I take the cleaned dataset, extract the hindi sentences one at a time and create the hindi vocabulary\n",
        "  english.processSentence(sample[1])                                               # now I take the cleaned dataset, extract the english sentences one at a time and create the english vocabulary\n",
        "\n",
        "\n",
        "print('--------------- vocabulary creation done-----------------')\n",
        "\n",
        "while True:pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------- vocabulary creation done-----------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-482fc8676b3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--------------- vocabulary creation done-----------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu8e8eIiJr3i",
        "outputId": "29b6d83c-dae4-4c94-eee3-245f61cd2d3d"
      },
      "source": [
        "print(english.name)                                                                                # printing out the attributes of the two objects hindi and english (of Class Vocavulary)\n",
        "print(hindi.name)\n",
        "print(english.total_words)                 # english vocabulary size\n",
        "print(hindi.total_words)                   # hindi vocabulary size\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english\n",
            "hindi\n",
            "27714\n",
            "37397\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC7HFWviGU4T"
      },
      "source": [
        "# **Defining the Encoder (LSTM) architecture**\n",
        "\n",
        "\n",
        "In the following cell the architecture of the Encoder is defined. I have used a GRU as a encoder. Please note I've made the following design choices.\n",
        "\n",
        "Number of recurrent layers- 1\n",
        "\n",
        "Number of features in the hidden state-512\n",
        "\n",
        "Size of the embedding vector created using nn.Embedding() - 512\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDJzTXG639z2"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size,num_layers=1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size                               # number of features in hidden state\n",
        "\n",
        "        self.num_layers=num_layers                                  # number of recurrent layers\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)        #this creates simple lookup table used to store word embeddings and retrieve them, note number of entries is same as size of hindi vocabulary\n",
        "                                                                      # and size of the embedding vector is same as hidden size \n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)                  # applies a GRU to an input sequence.\n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)                 # gives the foward pass, i.e. given input tensors get output tensors\n",
        "\n",
        "        output, hidden = self.gru(output, hidden)                                  \n",
        "        return output, hidden                                  \n",
        "\n",
        "    def initializeHiddenState(self):\n",
        "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)                 # fucntion to initialize the Hidden State"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU95VyZ_GSJv"
      },
      "source": [
        "# class Encoder(nn.Module):\n",
        "#     def __init__(self, input_size, hidden_size,num_layers=2):\n",
        "#         super(Encoder, self).__init__()\n",
        "#         self.hidden_size = hidden_size                               # number of features in hidden state\n",
        "\n",
        "#         self.num_layers=num_layers                                  # number of recurrent layers\n",
        "#         self.embedding = nn.Embedding(input_size, hidden_size)        #this creates simple lookup table used to store word embeddings and retrieve them, note number of entries is same as size of hindi vocabulary\n",
        "#                                                                       # and size of the embedding vector is same as hidden size \n",
        "#         self.lstm = nn.LSTM(hidden_size, hidden_size,num_layers)                  # applies a GRU to an input sequence.\n",
        "    \n",
        "#     def forward(self, input, hidden):\n",
        "#         output = self.embedding(input).view(1, 1, -1)                 # gives the foward pass, i.e. given input tensors get output tensors\n",
        "\n",
        "#         output, hidden,cell = self.lstm(output, hidden)                                  \n",
        "#         return output, hidden, cell                                  \n",
        "\n",
        "#     def initializeHiddenState(self):\n",
        "#         return torch.zeros(2, 1, self.hidden_size, device=device) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVqaFfgfGfgG"
      },
      "source": [
        "# **Defining the Decoder(LSTM) Architecture**\n",
        "\n",
        "\n",
        "In the following cell the architecture of the Decoder is defined. I have used a GRU as a decoder along with attention. Please note I've made the following design choices.\n",
        "\n",
        "Number of recurrent layers- 1\n",
        "\n",
        "Number of features in the hidden state-512\n",
        "\n",
        "Size of the embedding vector created using nn.Embedding() - 512\n",
        "\n",
        "Dropout probability- 0.4\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOgxG3EKMK3u"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, num_layers=1, dropout_p=0.4, max_length=50):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size                    # number of features in the hidden state\n",
        "        self.output_size = output_size                    # output size which is same as the size of the english vocabulary\n",
        "\n",
        "        self.dropout_p = dropout_p                       # dropout probability , used for regularization\n",
        "        self.max_length = max_length                        # max length of decoded woutput\n",
        "\n",
        "        self.num_layers=num_layers                                # number of recurrent layers\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)         #this creates simple lookup table used to store word embeddings and retrieve them, note number of entries is same as size of english vocabulary\n",
        "                                                                                    # size of embedding vector is same as hidden size\n",
        "        self.attention = nn.Linear(self.hidden_size * 2, self.max_length)                   #applies a linear transformation to the inputs\n",
        " \n",
        "        self.attention_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)                   # applies a linear transformation to the output\n",
        "\n",
        "        self.dropout = nn.Dropout(self.dropout_p)                                                 # introductes a dropout layer with dropout probability 0.4 \n",
        "\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)                                         # applies a GRU layer\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)                                        # applies a linear transformation\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "       \n",
        "        embedded = self.dropout(embedded)\n",
        "       \n",
        "        attention_weights = F.softmax(self.attention(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "       \n",
        "        applied = torch.bmm(attention_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))                      #  computes a batch matrix product of the given inputs as parameters\n",
        "\n",
        "        output = torch.cat((embedded[0], applied[0]), 1)\n",
        "      \n",
        "        output= F.relu(self.attention_combine(output).unsqueeze(0))\n",
        "\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attention_weights\n",
        "\n",
        "    def InitializaHiddenState(self):\n",
        "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alm1A5P8H5sw"
      },
      "source": [
        "# **Defining Utility funtions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F9-eazl5bV7"
      },
      "source": [
        "def createTensor(x):                                          # this fucntion serves the puporse of coverting a (hindi, english) sentence pair to a pair of tensors (hindi tensor, english tensor)\n",
        "  hin_wds=tokenize_hindi(x[0])          # tokenizing hindi sentence\n",
        "  eng_wds=tokenize_english(x[1])               # tokenizing english sentence\n",
        "  hin_idx=[]\n",
        "  eng_idx=[]\n",
        "\n",
        "  for wd in hin_wds:\n",
        "    hin_idx.append(hindi.word2index[wd]) # retriving the index correspoinding to the hindi word\n",
        "  hin_idx.append(1)                 # adding the indices to the list \n",
        "\n",
        "  for wd in eng_wds:\n",
        "    eng_idx.append(english.word2index[wd])             # retrieving the index corresponding to the english word\n",
        "  eng_idx.append(1)\n",
        "\n",
        "  hin_tensor= torch.tensor(hin_idx, dtype=torch.long, device=device).view(-1, 1)                #  creating the hidndi tensor\n",
        "  eng_tensor= torch.tensor(eng_idx, dtype=torch.long, device=device).view(-1, 1)                  # creating the endish tensor\n",
        "\n",
        "  return hin_tensor, eng_tensor\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgHgrTreQLv4"
      },
      "source": [
        "def sentence2tensor(x):            # fucntion to covert a hindi sentence to its corresponding tensor\n",
        "  hin_wds=tokenize_hindi(x)        # first tokenize sentence\n",
        "  hin_idx=[]\n",
        "  for wd in hin_wds:\n",
        "    hin_idx.append(hindi.word2index[wd])            # each token is coverted to its correspondeing index in the hindi vocabulary and added to list\n",
        "  hin_idx.append(1)\n",
        "  hin_tensor= torch.tensor(hin_idx, dtype=torch.long, device=device).view(-1, 1)            # creating the hindi tensor\n",
        "  \n",
        "\n",
        "  return hin_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNZheTwrJ462"
      },
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=50, tfr=0.5):\n",
        "\n",
        "    \n",
        "    hin_len = input_tensor.size(0)\n",
        "    eng_len= target_tensor.size(0)\n",
        "\n",
        "    enc_hidden=encoder.initializeHiddenState()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    enc_ops = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0                                  # initial loss set to 0\n",
        "\n",
        "    for x in range(hin_len):\n",
        "        enc_op, enc_hidden= encoder(input_tensor[x], enc_hidden)\n",
        "        enc_ops[x] = enc_op[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[0]], device=device)      # adding sos token to start, sos token is 0\n",
        "\n",
        "    decoder_hidden = enc_hidden                     \n",
        "\n",
        "    if random.random()<tfr:\n",
        "        for x in range(eng_len):                                                   # teacher forcing enabled i.e. the encoder uses the actual target word as the next input\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, enc_ops)\n",
        "            loss += criterion(decoder_output, target_tensor[x])                     # adding the decoder loss\n",
        "            decoder_input = target_tensor[x]                                       # using target word\n",
        "\n",
        "    else:\n",
        "        for x in range(eng_len):                                              # teacher forcing disabled, i.e. the decoder uses its predicted word as the next input\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, enc_ops)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()                     # using predicted word as the next input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[x])\n",
        "            if decoder_input.item() == 1:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    total_loss=loss.item()\n",
        "    actual_loss=total_loss/eng_len                           # the actual loss will be the total loss encountered by the decoder\n",
        "    return actual_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLDVKx--OkHw"
      },
      "source": [
        "def trainSeq2Seq(encoder, decoder, epochs, learning_rate):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)                     # using Adam optimizer for encoder training\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)                    # using Adam optimizer for decoder training\n",
        "\n",
        "    samples= [createTensor(random.choice(clean_data)) for i in range(epochs)]                      # getting a list of random training samples (tensors) from the data set. List size is the number of epochs, as we have used stochastic gradient descent\n",
        "    criterion = nn.CrossEntropyLoss()                                     # using Cross Entroppy loss function for training \n",
        "    count=1000\n",
        "\n",
        "    for i in range(0, epochs):\n",
        "        input_tensor = samples[i][0]                                   # note for every sample i.e sample[i], the 0th index stores the hindi tensor \n",
        "        target_tensor = samples[i][1]                                 # and the 1st index stores the english tensor\n",
        "\n",
        "        epoch_loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        if count==0:\n",
        "          count=1000\n",
        "          print(str(i),\" epochs completed.\")                                     # printing the status of training i.e number of epochs completed after every 1000 epochs\n",
        "        else:\n",
        "          count=count-1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLicPM7OKW-v"
      },
      "source": [
        "def translate(encoder, decoder, sentence, max_length=50):\n",
        "    with torch.no_grad():\n",
        "        input_tensor=sentence2tensor(sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden=encoder.initializeHiddenState()\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for x in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[x], encoder_hidden)\n",
        "            encoder_outputs[x] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[0]], device=device)  # adding sos token i.e. 0\n",
        "\n",
        "        decoder_hidden = encoder_hidden                           # last hidden state of the encoder is passed as the first hidden state of the decoder\n",
        "\n",
        "        decoded_words = []                           # to store the decoded words\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == 1:                            # if word predicted by decder is eos token i.e. 1\n",
        "                decoded_words.append('<eos>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(english.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        translated=' '.join(decoded_words[:-1])\n",
        "        return translated\n",
        "       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmqLfAdXDTVf"
      },
      "source": [
        "# def translate(encoder, decoder, sentence, max_length=50):\n",
        "#     with torch.no_grad():\n",
        "#         input_tensor=sentence2tensor(sentence)\n",
        "#         input_length = input_tensor.size()[0]\n",
        "#         encoder_hidden=encoder.initializeHiddenState()\n",
        "#         encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "#         for x in range(input_length):\n",
        "#             encoder_output, encoder_hidden = encoder(input_tensor[x], encoder_hidden)\n",
        "#             encoder_outputs[x] += encoder_output[0, 0]\n",
        "\n",
        "#         decoder_input = torch.tensor([[0]], device=device)  # adding sos token i.e. 0\n",
        "\n",
        "#         decoder_hidden = encoder_hidden                           # last hidden state of the encoder is passed as the first hidden state of the decoder\n",
        "\n",
        "#         decoded_words = []                           # to store the decoded words\n",
        "\n",
        "#         for di in range(max_length):\n",
        "#             decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "#             topv, topi = decoder_output.data.topk(2)\n",
        "#             print(10 ** topv.item() )\n",
        "#             if topi.item() == 1:                            # if word predicted by decder is eos token i.e. 1\n",
        "#                 decoded_words.append('<eos>')\n",
        "#                 break\n",
        "#             else:\n",
        "#                 decoded_words.append(english.index2word[topi.item()])\n",
        "\n",
        "#             decoder_input = topi.squeeze().detach()\n",
        "\n",
        "#         translated=' '.join(decoded_words[:-1])\n",
        "#         return translated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjqFIF1AH2G5"
      },
      "source": [
        "# **Training the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a62JR8QZLTeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe76eb88-cb3f-4462-cf6c-624cf06845c0"
      },
      "source": [
        "epochs=40000                                           # please note training swill take somewhere between 1-2 hrs\n",
        "\n",
        "hidden_size = 512\n",
        "learning_rate=0.0001                               \n",
        "\n",
        "encoder = Encoder(hindi.total_words, hidden_size).to(device)\n",
        "\n",
        "decoder = Decoder(hidden_size, english.total_words, dropout_p=0.4).to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('------------training seq2seq---------------')\n",
        "trainSeq2Seq(encoder,decoder, epochs,learning_rate)\n",
        "\n",
        "print('----------------training done------------------')\n",
        "#while True:pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------training seq2seq---------------\n",
            "1000  epochs completed.\n",
            "2001  epochs completed.\n",
            "3002  epochs completed.\n",
            "4003  epochs completed.\n",
            "5004  epochs completed.\n",
            "6005  epochs completed.\n",
            "7006  epochs completed.\n",
            "8007  epochs completed.\n",
            "9008  epochs completed.\n",
            "10009  epochs completed.\n",
            "11010  epochs completed.\n",
            "12011  epochs completed.\n",
            "13012  epochs completed.\n",
            "14013  epochs completed.\n",
            "15014  epochs completed.\n",
            "16015  epochs completed.\n",
            "17016  epochs completed.\n",
            "18017  epochs completed.\n",
            "19018  epochs completed.\n",
            "20019  epochs completed.\n",
            "21020  epochs completed.\n",
            "22021  epochs completed.\n",
            "23022  epochs completed.\n",
            "24023  epochs completed.\n",
            "25024  epochs completed.\n",
            "26025  epochs completed.\n",
            "27026  epochs completed.\n",
            "28027  epochs completed.\n",
            "29028  epochs completed.\n",
            "30029  epochs completed.\n",
            "31030  epochs completed.\n",
            "32031  epochs completed.\n",
            "33032  epochs completed.\n",
            "34033  epochs completed.\n",
            "35034  epochs completed.\n",
            "36035  epochs completed.\n",
            "37036  epochs completed.\n",
            "38037  epochs completed.\n",
            "39038  epochs completed.\n",
            "----------------training done------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp0cWTowRhFQ",
        "outputId": "7a588a68-4505-4e30-bc1a-0b5d2588c781"
      },
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "sentence=\"वे कहते हैं कि जहाज पर आप की जरूरत है।\"\n",
        "print(translate(encoder, decoder, sentence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "they re just need to you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVA6eKkd7zqp"
      },
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder, decoder, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKUUvLBn8KsD"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=50):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = sentence2tensor(sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initializeHiddenState()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[0]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == 1:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(english.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmGySzCT-_1q"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_Zw2u4M57lmR",
        "outputId": "899dbde5-09f4-461d-c9c9-be147bbf2a58"
      },
      "source": [
        "\n",
        "evaluateAndShowAttention(\"वे कहते हैं कि जहाज पर आप की जरूरत है।\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input = वे कहते हैं कि जहाज पर आप की जरूरत है।\n",
            "output = they re just need to you <EOS>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2357 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2375 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2325 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2361 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2340 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2376 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2306 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2367 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2332 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2366 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2346 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2352 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2310 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2368 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2370 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2404 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2357 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2375 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2325 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2361 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2340 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2376 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2306 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2367 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2332 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2366 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2346 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2352 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2310 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2368 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2370 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2404 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lc2CA_8HHW2"
      },
      "source": [
        "# **Generating the translated sentences of the development set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltOW4xvolsp-"
      },
      "source": [
        "with open('/content/drive/MyDrive/AssignmentNLP/week4/hindistatements.csv', newline='') as f1:                   # reading the input file (dev set) which has the hindi sentences to be translated\n",
        "    reader = csv.reader(f1)\n",
        "    hs = list(reader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9HztRiMqhsy",
        "outputId": "2802a33c-4225-43d6-c181-b09470568bbe"
      },
      "source": [
        "for i in range(0,10):\n",
        "  print(hs[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'id', 'hindi']\n",
            "['0', '0', 'कौन वे अपनी आस्तीन ऊपर है क्या अन्य तरकीबें जानता है?']\n",
            "['1', '1', 'हम कहानियों के ज़रिये अपने ज्ञान को आगे देते हैं।']\n",
            "['2', '2', 'फिर वे मुझे भी साथ लाते।']\n",
            "['3', '3', '- हाँ, दुर्भाग्य से.']\n",
            "['4', '4', 'मुलाक़ात नहीं हो पाई']\n",
            "['5', '5', 'और जब आप इस बारे में में सोचते हैं, कि हम संयुक्त राज्य अमेरिका के आर-पार चल रहे हैं डायनामाइट और पत्थर आरी के साथ सिर्फ इसलिए की एक एल्गोरिथ्म सौदा कर सके तीन मिक्रोसेकांड्स तेज, एक संचार ढांचे के लिए जो कभी किसी इंसान को नहीं पता चलेगा, जो प्रकट भाग्य का एक प्रकार है और हमेशा एक नया मोर्चा तलाशेगा .']\n",
            "['6', '6', 'मैं कभी इतना खुश या आराम से कहीं भी नहीं रहा.']\n",
            "['7', '7', 'और हमने यहाँ सीखा की: अधिकतम कूड़ा एक बहुत अच्छे टैको ब्रांड से आ रहा था']\n",
            "['8', '8', '♫ हम 12 अरब प्रकाश वर्ष से किनारों से दूर हैं ♫ ♫ यह एक अनुमान है ♫ ♫ कोई भी कभी नहीं कह सकता है कि यह सच है ♫ ♫, लेकिन मैं जानती हूँ कि मैं हमेशा तुम्हारे साथ रहूंगी ♫']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdZPA-f-qs3n"
      },
      "source": [
        "hs_list=hs[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oM3HzuNcqyCF",
        "outputId": "e0cae81c-3ceb-4e10-c98c-6d943ae9ee81"
      },
      "source": [
        "print(len(hs_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h55AU2IDq4JL",
        "outputId": "d451c1af-9b20-46c1-88ed-b212a3b5692f"
      },
      "source": [
        "hindi_list=[]\n",
        "c=0\n",
        "for i in hs_list:                               # extracting each hindi sentence, along with standardizing it and dealing with out of vocabulary tokens\n",
        "  s=i[2]\n",
        "  s=standardize(s)\n",
        "  word_list=tokenize_hindi(s)\n",
        "  #word_len=len(word_list)\n",
        "  s=''\n",
        "  for wd in word_list:\n",
        "    if wd not in hindi.word2index:\n",
        "      continue\n",
        "    else:\n",
        "      s=s+wd+' '\n",
        "  word_list=tokenize_hindi(s)\n",
        "  word_len=len(word_list)\n",
        "  if word_len>30:\n",
        "    s=''\n",
        "    for j in range(0,30):\n",
        "      s=s+word_list[j]+' '\n",
        "    c=c+1\n",
        "  s.strip()\n",
        "  hindi_list.append(s)\n",
        "\n",
        "print(c)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keZkwtqwr5NG",
        "outputId": "6743ef96-713c-40a9-b48f-851dbfc9119a"
      },
      "source": [
        "for i in range(0,10):\n",
        "  print(hindi_list[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "कौन वे अपनी आस्तीन ऊपर है क्या अन्य तरकीबें जानता है \n",
            "हम कहानियों के ज़रिये अपने ज्ञान को आगे देते हैं । \n",
            "फिर वे मुझे भी साथ लाते । \n",
            "हाँ दुर्भाग्य से \n",
            "मुलाक़ात नहीं हो पाई \n",
            "और जब आप इस बारे में में सोचते हैं कि हम संयुक्त राज्य अमेरिका के आरपार चल रहे हैं डायनामाइट और पत्थर आरी के साथ सिर्फ इसलिए की एक एल्गोरिथ्म \n",
            "मैं कभी इतना खुश या आराम से कहीं भी नहीं रहा \n",
            "और हमने यहाँ सीखा की अधिकतम कूड़ा एक बहुत अच्छे ब्रांड से आ रहा था \n",
            "♫ हम 12 अरब प्रकाश वर्ष से किनारों से दूर हैं ♫ ♫ यह एक अनुमान है ♫ ♫ कोई भी कभी नहीं कह सकता है कि यह सच है \n",
            "उन्हें गोली मार \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9kr5RfGudsu",
        "outputId": "4618ed55-6000-45b9-9631-378213fb790f"
      },
      "source": [
        "len(hindi_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnipuL3cuZV_",
        "outputId": "8fd56e74-284b-462b-cca8-ba15168d0213"
      },
      "source": [
        "op=[] \n",
        "c1=0                                                          # this is a simple white space de-tokenizer implemented by me which is being used for processing the translated english sentence\n",
        "for i in range(0,len(hindi_list)):\n",
        "  sentence=hindi_list[i]\n",
        "  if len(sentence)==0:\n",
        "    c1=c1+1\n",
        "    op.append('')\n",
        "    continue\n",
        "  translated=translate(encoder,decoder, sentence)\n",
        "  op.append(translated.strip())                              # adding the translated sentence to the list of outputs i.e. op\n",
        "print(c1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QZBsTpw3m8x",
        "outputId": "7482e897-4905-42ad-b193-7307382531b7"
      },
      "source": [
        "for i in range(0,10):\n",
        "  print(op[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "what they they what they have their own\n",
            "we have to our our our\n",
            "then they me me with me\n",
            "yeah yeah it\n",
            "no not nt\n",
            "and when you re about this this that we we we we the\n",
            "i m not not or or or not\n",
            "and we were the the of the of a a a a of\n",
            "i ca nt have a of of the of the that that is not nt that that\n",
            "they did them\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKkruz7M5WC_",
        "outputId": "dd4f61f4-4125-41b5-c696-dead6d308315"
      },
      "source": [
        "print(len(op))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nQTHecUvWgl"
      },
      "source": [
        "with open('/content/drive/MyDrive/AssignmentNLP/answer.txt', 'w') as f:                                      # creating and witing the translated english sentences to the output file\n",
        "    for item in op:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjzqRjpLY3TW"
      },
      "source": [
        "torch.save(encoder.state_dict(), '/content/drive/MyDrive/AssignmentNLP/model/ph4sub1-enc')\n",
        "torch.save(decoder.state_dict(), '/content/drive/MyDrive/AssignmentNLP/model/ph4sub1-dec')\n",
        "\n",
        "\n",
        "# model = TheModelClass(*args, **kwargs)\n",
        "# model.load_state_dict(torch.load(PATH))\n",
        "# model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0onoYIoubxW"
      },
      "source": [
        "while True:pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sYHaNmv5qwz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}